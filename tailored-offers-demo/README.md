# Tailored Offers - Agentic AI Demo

This demo application showcases a 6-agent architecture for American Airlines' Tailored Offers system, demonstrating how agentic AI adds value beyond ML models alone.

## Quick Start

### Option 1: Docker (Recommended)

```bash
# Build and run everything (mock mode - no API keys needed)
docker-compose up --build

# Open browser to http://localhost:3000
```

**With LLM Reasoning (optional):**
```bash
# Create .env file with your API key
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY or ANTHROPIC_API_KEY

# Run with LLM
docker-compose up --build
```

### Option 2: Local Development

**Terminal 1 - Start API:**
```bash
# Install Python dependencies
pip install -r requirements.txt
pip install -r api/requirements.txt

# Optional: Set API key for real LLM reasoning
export OPENAI_API_KEY=sk-your-key-here
# OR
export ANTHROPIC_API_KEY=sk-ant-your-key-here

# Start the API server
python -m uvicorn api.main:app --port 8000
```

**Terminal 2 - Start Frontend:**
```bash
cd frontend
npm install
npm run dev

# Open browser to http://localhost:5173
```

### Option 3: CLI Demo (No UI)

```bash
pip install -r requirements.txt
python run_demo.py --pnr ABC123
python run_demo.py --all  # Run all scenarios
```

---

## Hybrid Architecture: Rules + LLM Reasoning

This demo showcases a **hybrid approach** - combining fast rules-based agents with LLM-powered reasoning agents:

### Agent Types

| Agent | Type | Why |
|-------|------|-----|
| **Customer Intelligence** | âš¡ Rules | Fast eligibility checks, deterministic |
| **Flight Optimization** | âš¡ Rules | Inventory analysis, business rules |
| **Offer Orchestration** | ğŸ§  LLM | Strategic reasoning about optimal offers |
| **Personalization** | ğŸ§  LLM | GenAI for personalized messaging |
| **Channel & Timing** | âš¡ Rules | Channel selection rules |
| **Measurement** | âš¡ Rules | Deterministic A/B assignment |

### Why Hybrid?

| Benefit | Explanation |
|---------|-------------|
| **Performance** | Rules agents run in milliseconds |
| **Intelligence** | LLM agents handle complex decisions |
| **Cost Efficient** | Only use LLM where it adds value |
| **Graceful Degradation** | Works without API keys (mock mode) |

### LLM Configuration

```bash
# Option 1: OpenAI
export OPENAI_API_KEY=sk-your-key-here

# Option 2: Anthropic (Claude)
export ANTHROPIC_API_KEY=sk-ant-your-key-here

# No key = Mock mode (simulated LLM responses)
```

---

## What This Demo Shows

### MVP1 Key Messages

1. **Existing Systems Unchanged** - Agents call existing systems via MCP Tools
2. **Hybrid Architecture** - Rules for speed, LLM for intelligence
3. **Full Explainability** - Every decision has a traceable reasoning chain

### The 5 Demo Scenarios

| PNR | Customer | What It Demonstrates | Expected Outcome |
|-----|----------|---------------------|------------------|
| **ABC123** | Sarah (Gold) | Full happy path - all 6 agents | âœ… Business @ $171 |
| **XYZ789** | John (Platinum Pro) | Behavioral adaptation - price adjustment | âœ… Business @ $165 |
| **LMN456** | Emily (Exec Platinum) | High-value treatment - premium channel | âœ… Business @ $770 |
| **DEF321** | Michael (General) | Cold start + inventory constraint | âŒ No offer |
| **GHI654** | Lisa (Platinum) | Suppression - customer protection | âŒ No offer |

### Recommended Demo Flow

1. **Start with ABC123** - Show the full happy path with all 6 agents
2. **Show GHI654** - Pipeline stops early (suppressed customer)
3. **Show DEF321** - Cold start handling + inventory awareness
4. **Show XYZ789** - Behavioral adaptation (follow-up pricing)
5. **Show LMN456** - High-value treatment

---

## Architecture: State-Based Choreography

**Key Concept**: There is NO "boss" agent. Instead, agents pass a **Shared State Object** like a baton in a relay race.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STATE-BASED CHOREOGRAPHY                             â”‚
â”‚         LangGraph manages the State Object + Agent Sequence              â”‚
â”‚                                                                          â”‚
â”‚   ğŸ“¦ State â†’ [Agent 1] â†’ ğŸ“¦ State â†’ [Agent 2] â†’ ğŸ“¦ State â†’ ...          â”‚
â”‚                                                                          â”‚
â”‚   â€¢ Each agent reads state, does ONE focused job, updates state          â”‚
â”‚   â€¢ Short, accurate prompts (no complex "orchestrator" prompt)           â”‚
â”‚   â€¢ Easy to test, debug, and modify individual agents                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AGENT LAYER (Reasoning)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Customer â”‚â†’â”‚  Flight  â”‚â†’â”‚  Offer   â”‚â†’â”‚ Personal-â”‚â†’â”‚Channelâ”‚â†’â”‚Measureâ”‚ â”‚
â”‚  â”‚  Intel   â”‚ â”‚  Optim.  â”‚ â”‚  Orch.   â”‚ â”‚ ization  â”‚ â”‚Timing â”‚ â”‚Learn  â”‚ â”‚
â”‚  â”‚  âš¡Rules â”‚ â”‚  âš¡Rules â”‚ â”‚  ğŸ§  LLM  â”‚ â”‚  ğŸ§  LLM  â”‚ â”‚âš¡Rulesâ”‚ â”‚âš¡Rulesâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                          Agents call MCP Tools
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MCP TOOL LAYER (Interface)                       â”‚
â”‚   get_customer_profile() â”‚ get_propensity_scores() â”‚ get_inventory()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXISTING SYSTEMS (UNCHANGED)                          â”‚
â”‚        AADV DB    â”‚    ML Model    â”‚    DCSID    â”‚    RM Engine         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why State-Based Choreography (not Central Orchestration)?

| Central Orchestration âŒ | State-Based Choreography âœ… |
|--------------------------|----------------------------|
| One "supervisor" agent calls all shots | Each agent has ONE focused job |
| Requires huge, complex prompts | Short, accurate prompts |
| Single point of failure | Agents are independent & testable |
| Hard to debug and maintain | Easy to add/modify agents |

### The 6 Agents

| Agent | Type | Purpose | MCP Tools Used |
|-------|------|---------|----------------|
| **Customer Intelligence** | âš¡ Rules | Eligibility & segmentation | `get_customer_profile`, `get_suppression_status` |
| **Flight Optimization** | âš¡ Rules | Capacity analysis | `get_flight_inventory`, `get_pricing` |
| **Offer Orchestration** | ğŸ§  LLM | Multi-offer arbitration | `get_propensity_scores`, `get_pricing` |
| **Personalization** | ğŸ§  LLM | GenAI messaging | `get_customer_profile` |
| **Channel & Timing** | âš¡ Rules | Delivery optimization | `get_consent_status`, `get_engagement_history` |
| **Measurement** | âš¡ Rules | A/B testing & feedback | `assign_experiment` |

---

## Why Agents vs Rule Engine?

| Rule Engine | LLM Agent Reasoning |
|-------------|---------------------|
| `if P(buy) > 0.5: send_offer()` | "Given price sensitivity + inventory needs, Business EV ($122) > MCE EV ($29)" |
| Fixed thresholds | Holistic context-aware decisions |
| Can't explain WHY | Full audit trail with reasoning |
| Breaks on edge cases | Graceful fallbacks |

**Key insight:** ML gives you a score. LLM Agents give you decisions + explanations.

---

## Feedback Loop Patterns

### How Do Agents Learn?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FORWARD FLOW                                      â”‚
â”‚   Agent 1 â†’ Agent 2 â†’ Agent 3 (LLM) â†’ Agent 4 â†’ Decision                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘                                  â”‚
                    â”‚         FEEDBACK LOOPS           â”‚
                    â”‚                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Intra-Pipeline    â”‚    Post-Decision      â”‚    Continuous Learning    â”‚
â”‚  (Agent Conflict)  â”‚    (Customer Action)  â”‚    (ML Retraining)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Intra-Pipeline Feedback (Agent Disagreement)

**Question:** *"What if Agent 3 disagrees with Agent 2's recommendation?"*

| Mechanism | How It Works |
|-----------|--------------|
| **Shared State** | Each agent sees all prior decisions + reasoning in the state object |
| **LangGraph Routing** | Conditional edges can loop back to earlier agents if conflict detected |
| **Example** | Offer Orchestration recommends Business, but Channel Agent sees no email consent â†’ State updated â†’ Could trigger re-evaluation |

**Code Pattern (LangGraph):**
```python
# Conditional routing based on agent output
workflow.add_conditional_edges(
    "channel_timing",
    should_reconsider_offer,  # Returns "offer_orchestration" if conflict
    {
        "continue": "measurement",
        "reconsider": "offer_orchestration"  # Loop back!
    }
)
```

### 2. Post-Decision Feedback (Customer Response)

**Question:** *"How do agents learn from what customers actually do?"*

| Stage | What Happens |
|-------|--------------|
| **Offer Sent** | Measurement Agent assigns tracking ID, experiment group |
| **Customer Action** | Accept / Reject / Ignore â†’ logged to analytics |
| **Outcome Captured** | Revenue, conversion, engagement metrics stored |
| **Model Update** | P(buy) scores refined with new outcome data |

### 3. Continuous Learning Cycle

```
Offer Sent â†’ Customer Action â†’ Outcome Logged â†’ ML Retrained â†’ Better P(buy) â†’ Better Decisions
     â†‘                                                                              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**

| Component | Role in Feedback |
|-----------|------------------|
| **Measurement Agent** | Assigns A/B test groups, generates tracking IDs |
| **Analytics Pipeline** | Captures outcomes (accept/reject/revenue) |
| **ML Training** | Retrains propensity models with new data |
| **LLM Context** | Can include historical outcomes in prompts for better reasoning |

### Future: Reinforcement Learning

For MVP-2+, agents could learn directly from outcomes:

```python
# Pseudo-code for RL feedback
def update_agent_policy(offer_made, customer_response, revenue):
    if customer_response == "accepted":
        reward = revenue
    elif customer_response == "rejected":
        reward = -cost_of_rejection
    else:
        reward = -cost_of_annoyance

    agent.update_policy(reward)  # Reinforce good decisions
```

---

## Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Workflow Orchestration** | LangGraph | Agent graph, conditional routing |
| **LLM Integration** | OpenAI / Anthropic | Reasoning & generation |
| **Durable Execution** | Temporal (future) | Reliability, retries |
| **Backend** | FastAPI + SSE | Real-time streaming |
| **Frontend** | React + Vite + Tailwind | Modern UI |
| **Deployment** | Docker Compose | Easy setup |

---

## Project Structure

```
tailored-offers-demo/
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py            # API endpoints + SSE streaming
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/              # React + Vite UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ hooks/         # SSE hook
â”‚   â”‚   â””â”€â”€ types/         # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ agents/                # The 6 agents
â”‚   â”œâ”€â”€ customer_intelligence.py   # âš¡ Rules
â”‚   â”œâ”€â”€ flight_optimization.py     # âš¡ Rules
â”‚   â”œâ”€â”€ offer_orchestration.py     # ğŸ§  LLM
â”‚   â”œâ”€â”€ personalization.py         # ğŸ§  LLM
â”‚   â”œâ”€â”€ channel_timing.py          # âš¡ Rules
â”‚   â”œâ”€â”€ measurement_learning.py    # âš¡ Rules
â”‚   â”œâ”€â”€ llm_service.py             # LLM provider abstraction
â”‚   â”œâ”€â”€ state.py
â”‚   â””â”€â”€ workflow.py
â”œâ”€â”€ tools/                 # Data access (MCP tool simulation)
â”‚   â””â”€â”€ data_tools.py
â”œâ”€â”€ data/                  # Mock data (JSON)
â”œâ”€â”€ docker-compose.yml     # Run everything with Docker
â”œâ”€â”€ .env.example           # Environment variable template
â”œâ”€â”€ run_demo.py           # CLI entry point
â””â”€â”€ requirements.txt
```

---

## Deployment Options

### For Demo on Organization Laptop

**Option A: Docker (Easiest)**
```bash
# Requires: Docker Desktop installed
git clone <repo-url>
cd tailored-offers-demo

# Without LLM (mock mode)
docker-compose up --build

# With LLM reasoning
cp .env.example .env
# Edit .env and add OPENAI_API_KEY or ANTHROPIC_API_KEY
docker-compose up --build

# Open http://localhost:3000
```

**Option B: Manual Setup**
```bash
# Requires: Python 3.10+, Node.js 18+
git clone <repo-url>
cd tailored-offers-demo

# Backend
pip install -r requirements.txt
pip install -r api/requirements.txt

# Optional: Enable LLM
export OPENAI_API_KEY=sk-your-key

# Frontend
cd frontend && npm install && cd ..

# Run (2 terminals)
python -m uvicorn api.main:app --port 8000
cd frontend && npm run dev
```

---

## Key Business Value

1. **Immediate (Tier 1)**: Arbitration, personalization, channel optimization at T-72/48/24 hours
2. **Future (Tier 2)**: Real-time scenarios (lounge offers, IROP handling) when infrastructure exists

The agent architecture built for MVP-1 is **future-proof** - same architecture enables real-time scenarios when business invests in infrastructure.
