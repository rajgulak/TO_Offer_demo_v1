version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      # Mount data for easy updates without rebuild
      - ./data:/app/data:ro
    environment:
      # LLM Configuration - set one of these for real LLM reasoning
      # If neither is set, demo runs in mock mode with simulated responses
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - TAILORED_OFFERS_LLM_PROVIDER=${TAILORED_OFFERS_LLM_PROVIDER:-}
      # Disable extra LLM calls for reasoning explanations (faster demo)
      - USE_DYNAMIC_REASONING=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      api:
        condition: service_healthy
    environment:
      - VITE_API_URL=http://localhost:8000

# For development, you can also run:
# docker-compose up api  (just the API)
# Then run frontend locally with: cd frontend && npm run dev
